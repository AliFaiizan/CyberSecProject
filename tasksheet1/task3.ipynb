{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c86f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 training files: ['../hai-21.03/train1.csv']\n",
      "Found 1 test files: ['../hai-21.03/test1.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from typing import List, Tuple\n",
    "\n",
    "# File paths and column definitions\n",
    "hai_21_train_files = sorted(glob(\"../hai-21.03/train1.csv\"))\n",
    "hai_21_test_files = sorted(glob(\"../hai-21.03/test1.csv\"))\n",
    "hai_21_attack_cols = ['attack', 'attack_P1', 'attack_P2', 'attack_P3']\n",
    "\n",
    "print(f\"Found {len(hai_21_train_files)} training files: {hai_21_train_files}\")  \n",
    "print(f\"Found {len(hai_21_test_files)} test files: {hai_21_test_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2aa8b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilter:\n",
    "    \"\"\"Bloom filter implementation for n-gram anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, size=1_000_000, k=5):\n",
    "        self.size = size\n",
    "        self.k = k\n",
    "        self.bloom = np.zeros(size, dtype=bool)\n",
    "        \n",
    "    def _hashes(self, item: str) -> List[int]:\n",
    "        \"\"\"Generate k hash values for an item\"\"\"\n",
    "        return [\n",
    "            int(hashlib.sha1((str(seed) + item).encode()).hexdigest(), 16) % self.size\n",
    "            for seed in range(self.k)\n",
    "        ]\n",
    "    \n",
    "    def add(self, item: str):\n",
    "        \"\"\"Add item to bloom filter\"\"\"\n",
    "        for h in self._hashes(item):\n",
    "            self.bloom[h] = True\n",
    "    \n",
    "    def check(self, item: str) -> bool:\n",
    "        \"\"\"Check if item might be in the filter\"\"\"\n",
    "        return all(self.bloom[h] for h in self._hashes(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "de90ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(train_files: List[str], test_files: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Step 1: Load & Clean Data\n",
    "    - Read all train CSVs for HAI 21.03\n",
    "    - Drop timestamp and attack labels\n",
    "    - Remove rows where Attack == 1 (use only normal data for training)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Step 1: Loading & Cleaning Data ===\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_dfs = []\n",
    "    for file in train_files:\n",
    "        print(f\"Loading {file}...\")\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Remove attack rows (keep only normal data for training)\n",
    "        if 'attack' in df.columns:\n",
    "            normal_mask = df['attack'] == 0\n",
    "            df = df[normal_mask]\n",
    "        \n",
    "        train_dfs.append(df)\n",
    "    \n",
    "    # Load test data\n",
    "    test_dfs = []\n",
    "    for file in test_files:\n",
    "        print(f\"Loading {file}...\")\n",
    "        df = pd.read_csv(file)\n",
    "        test_dfs.append(df)\n",
    "    \n",
    "    # Combine all data\n",
    "    train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "    test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "    \n",
    "    # Drop timestamp and attack columns\n",
    "    cols_to_drop = ['time'] + [col for col in hai_21_attack_cols if col in train_df.columns]\n",
    "    train_df = train_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    test_df = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    print(f\"Final training data shape: {train_df.shape}\")\n",
    "    print(f\"Final test data shape: {test_df.shape}\") \n",
    "    \n",
    "    # Handle NaN values\n",
    "    train_df = train_df.fillna(method='ffill').fillna(0)\n",
    "    test_df = test_df.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    # train_df = train_df.sample(n=1000)\n",
    "    # test_df = test_df.sample(n=1000)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "77171145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_quantize(train_data: pd.DataFrame, test_data: pd.DataFrame, Q: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Step 2: Normalize & Quantize\n",
    "    - Use z-score normalization instead of min-max\n",
    "    - Quantize into discrete bins (0 to Q-1)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 2: Normalizing & Quantizing (Q={Q}) ===\")\n",
    "    train_data = train_data.round()\n",
    "    test_data = test_data.round()\n",
    "    # Calculate z-score normalization parameters from training data only\n",
    "    mean_vals = train_data.mean()\n",
    "    std_vals = train_data.std()\n",
    "    \n",
    "    print(f\"Data ranges before normalization:\")\n",
    "    print(f\"  Train: min={train_data.min().min():.2f}, max={train_data.max().max():.2f}\")\n",
    "    print(f\"  Test: min={test_data.min().min():.2f}, max={test_data.max().max():.2f}\")\n",
    "    \n",
    "    # Z-score normalization\n",
    "    train_normalized = (train_data - mean_vals) / std_vals\n",
    "    test_normalized = (test_data - mean_vals) / std_vals\n",
    "    \n",
    "    # Handle any remaining NaN/inf values\n",
    "    train_normalized = train_normalized.fillna(0)\n",
    "    test_normalized = test_normalized.fillna(0)\n",
    "    train_normalized = train_normalized.replace([np.inf, -np.inf], 0)\n",
    "    test_normalized = test_normalized.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Clip extreme values to reasonable range (e.g., -3 to 3 standard deviations)\n",
    "    train_normalized = np.clip(train_normalized, -3, 3)\n",
    "    test_normalized = np.clip(test_normalized, -3, 3)\n",
    "    \n",
    "    # Rescale to [0, 1] range for quantization\n",
    "    # Map [-3, 3] to [0, 1]\n",
    "    train_scaled = (train_normalized + 3) / 6\n",
    "    test_scaled = (test_normalized + 3) / 6\n",
    "    \n",
    "    # Quantize into discrete bins [0, Q-1]\n",
    "    train_quantized = np.floor(train_scaled * Q).astype(int)\n",
    "    test_quantized = np.floor(test_scaled * Q).astype(int)\n",
    "    \n",
    "    # Ensure values are in valid range\n",
    "    train_quantized = np.clip(train_quantized, 0, Q-1)\n",
    "    test_quantized = np.clip(test_quantized, 0, Q-1)\n",
    "    \n",
    "    print(f\"Quantized ranges:\")\n",
    "    print(f\"  Train: min={train_quantized.min()}, max={train_quantized.max()}\")\n",
    "    print(f\"  Test: min={test_quantized.min()}, max={test_quantized.max()}\")\n",
    "    \n",
    "    return train_quantized, test_quantized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db29e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_state_strings(quantized_data: np.ndarray) -> List[str]:\n",
    "    \"\"\"\n",
    "    Step 3: Build State String per Time Step\n",
    "    Convert each time row into a state string\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 3: Building State Strings ===\")\n",
    "    state_strings = []\n",
    "    for i, row in enumerate(quantized_data):\n",
    "        state = \"_\".join(map(str, row))\n",
    "        state_strings.append(state)\n",
    "    \n",
    "    #print(f\"Generated {len(state_strings)} state strings\")\n",
    "    return state_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a27e485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(state_strings: List[str], n: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate N-grams\n",
    "    Slide a window of size n over the sequence of state strings\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 4: Generating {n}-grams ===\")\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(state_strings) - n + 1):\n",
    "        sequence = state_strings[i:i+n]\n",
    "        ngram = \"→\".join(sequence)\n",
    "        ngrams.append(ngram)\n",
    "        \n",
    "        if i < 3:  # Show first few examples\n",
    "            print(f\"  N-gram {i}: {ngram[:100]}...\")\n",
    "    \n",
    "    print(f\"Generated {len(ngrams)} n-grams\")\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "52800297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bloom_filter(ngrams: List[str], M: int = 1_000_000, k: int = 5) -> BloomFilter:\n",
    "    \"\"\"\n",
    "    Hash and Store in Bloom Filter\n",
    "    Train the bloom filter with normal n-grams\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training Bloom Filter (M={M}, k={k}) ===\")\n",
    "    \n",
    "    bloom = BloomFilter(size=M, k=k)\n",
    "    \n",
    "    for i, ngram in enumerate(ngrams):\n",
    "        bloom.add(ngram)\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Added {i+1}/{len(ngrams)} n-grams\")\n",
    "    \n",
    "    print(f\"Bloom filter training completed!\")\n",
    "    return bloom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "344a6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(test_ngrams: List[str], bloom_filter: BloomFilter) -> Tuple[List[bool], float]:\n",
    "    \"\"\"\n",
    "    Detect anomalies using the trained bloom filter\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Anomaly Detection ===\")\n",
    "    \n",
    "    anomaly_scores = []\n",
    "    for ngram in test_ngrams:\n",
    "        is_normal = bloom_filter.check(ngram)\n",
    "        anomaly_scores.append(not is_normal)  # Anomaly if not in bloom filter\n",
    "    \n",
    "    anomaly_rate = sum(anomaly_scores) / len(anomaly_scores)\n",
    "    print(f\"Anomaly rate: {anomaly_rate:.4f} ({sum(anomaly_scores)}/{len(anomaly_scores)})\")\n",
    "    \n",
    "    return anomaly_scores, anomaly_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fd9cc",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c9ea1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Loading & Cleaning Data ===\n",
      "Loading ../hai-21.03/train1.csv...\n",
      "Loading ../hai-21.03/test1.csv...\n",
      "Final training data shape: (216001, 79)\n",
      "Final test data shape: (43201, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Faizan\\AppData\\Local\\Temp\\ipykernel_50396\\1179842295.py:43: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.fillna(method='ffill').fillna(0)\n",
      "C:\\Users\\Ali Faizan\\AppData\\Local\\Temp\\ipykernel_50396\\1179842295.py:44: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.fillna(method='ffill').fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and clean data\n",
    "train_df, test_df = load_and_clean_data(hai_21_train_files, hai_21_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "644ef9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Normalizing & Quantizing (Q=10) ===\n",
      "Data ranges before normalization:\n",
      "  Train: min=-288.00, max=540833.00\n",
      "  Test: min=-288.00, max=540833.00\n",
      "Quantized ranges:\n",
      "  Train: min=P1_B2004      5\n",
      "P1_B2016      4\n",
      "P1_B3004      1\n",
      "P1_B3005      0\n",
      "P1_B4002      3\n",
      "             ..\n",
      "P4_ST_LD      0\n",
      "P4_ST_PO      0\n",
      "P4_ST_PS      4\n",
      "P4_ST_PT01    0\n",
      "P4_ST_TT01    0\n",
      "Length: 79, dtype: int64, max=P1_B2004      5\n",
      "P1_B2016      8\n",
      "P1_B3004      9\n",
      "P1_B3005      6\n",
      "P1_B4002      8\n",
      "             ..\n",
      "P4_ST_LD      9\n",
      "P4_ST_PO      9\n",
      "P4_ST_PS      9\n",
      "P4_ST_PT01    9\n",
      "P4_ST_TT01    6\n",
      "Length: 79, dtype: int64\n",
      "  Test: min=P1_B2004      5\n",
      "P1_B2016      4\n",
      "P1_B3004      4\n",
      "P1_B3005      2\n",
      "P1_B4002      3\n",
      "             ..\n",
      "P4_ST_LD      0\n",
      "P4_ST_PO      0\n",
      "P4_ST_PS      4\n",
      "P4_ST_PT01    0\n",
      "P4_ST_TT01    0\n",
      "Length: 79, dtype: int64, max=P1_B2004      5\n",
      "P1_B2016      8\n",
      "P1_B3004      5\n",
      "P1_B3005      6\n",
      "P1_B4002      6\n",
      "             ..\n",
      "P4_ST_LD      9\n",
      "P4_ST_PO      9\n",
      "P4_ST_PS      9\n",
      "P4_ST_PT01    9\n",
      "P4_ST_TT01    6\n",
      "Length: 79, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_quantized, test_quantized = normalize_and_quantize(train_df, test_df, Q=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "056191bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1_B2004</th>\n",
       "      <th>P1_B2016</th>\n",
       "      <th>P1_B3004</th>\n",
       "      <th>P1_B3005</th>\n",
       "      <th>P1_B4002</th>\n",
       "      <th>P1_B4005</th>\n",
       "      <th>P1_B400B</th>\n",
       "      <th>P1_B4022</th>\n",
       "      <th>P1_FCV01D</th>\n",
       "      <th>P1_FCV01Z</th>\n",
       "      <th>...</th>\n",
       "      <th>P4_HT_PO</th>\n",
       "      <th>P4_HT_PS</th>\n",
       "      <th>P4_LD</th>\n",
       "      <th>P4_ST_FD</th>\n",
       "      <th>P4_ST_GOV</th>\n",
       "      <th>P4_ST_LD</th>\n",
       "      <th>P4_ST_PO</th>\n",
       "      <th>P4_ST_PS</th>\n",
       "      <th>P4_ST_PT01</th>\n",
       "      <th>P4_ST_TT01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   P1_B2004  P1_B2016  P1_B3004  P1_B3005  P1_B4002  P1_B4005  P1_B400B  \\\n",
       "0         5         4         4         3         8         6         6   \n",
       "1         5         4         4         3         8         6         6   \n",
       "2         5         4         4         3         8         6         6   \n",
       "3         5         4         4         3         8         6         6   \n",
       "4         5         4         4         3         8         6         6   \n",
       "\n",
       "   P1_B4022  P1_FCV01D  P1_FCV01Z  ...  P4_HT_PO  P4_HT_PS  P4_LD  P4_ST_FD  \\\n",
       "0         7          6          6  ...         3         4      3         5   \n",
       "1         7          6          6  ...         3         4      3         5   \n",
       "2         7          6          6  ...         3         4      3         5   \n",
       "3         7          6          6  ...         3         4      3         5   \n",
       "4         7          6          6  ...         3         4      3         5   \n",
       "\n",
       "   P4_ST_GOV  P4_ST_LD  P4_ST_PO  P4_ST_PS  P4_ST_PT01  P4_ST_TT01  \n",
       "0          3         3         3         4           5           5  \n",
       "1          3         3         3         4           5           5  \n",
       "2          3         3         3         4           5           5  \n",
       "3          3         3         3         4           5           5  \n",
       "4          3         3         3         4           5           5  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quantized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c98afd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 3: Building State Strings ===\n",
      "\n",
      "=== Step 3: Building State Strings ===\n"
     ]
    }
   ],
   "source": [
    "train_states = build_state_strings(train_quantized.values) \n",
    "test_states = build_state_strings(test_quantized.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a306e75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5_4_4_3_8_6_6_7_6_6_3_3_3_3_3_3_6_6_2_3_2_2_4_2_2_5_5_4_4_5_5_5_5_5_5_5_3_4_5_5_5_2_5_4_5_5_5_5_5_5_5_5_5_5_5_5_6_6_5_4_8_7_3_5_5_5_5_5_3_3_4_3_5_3_3_3_4_5_5'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "45d19439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 4: Generating 2-grams ===\n",
      "  N-gram 0: 5_4_4_3_8_6_6_7_6_6_3_3_3_3_3_3_6_6_2_3_2_2_4_2_2_5_5_4_4_5_5_5_5_5_5_5_3_4_5_5_5_2_5_4_5_5_5_5_5_5_...\n",
      "  N-gram 1: 5_4_4_3_8_6_6_7_6_6_3_3_3_3_3_3_6_6_2_3_2_2_4_2_2_5_5_4_4_5_5_5_5_5_5_5_3_4_5_5_5_3_5_4_5_5_5_5_5_5_...\n",
      "  N-gram 2: 5_4_4_3_8_6_6_7_6_6_3_3_3_3_3_3_6_6_2_3_2_2_4_2_2_5_5_4_4_5_5_5_5_5_5_5_3_4_5_5_5_6_5_4_5_5_5_5_5_5_...\n",
      "Generated 216000 n-grams\n",
      "\n",
      "=== Step 4: Generating 2-grams ===\n",
      "  N-gram 0: 5_8_5_2_6_6_6_7_6_6_3_3_2_2_3_4_6_6_2_2_3_4_5_3_2_5_5_4_4_5_5_5_5_5_5_5_3_3_5_5_5_5_5_7_5_5_5_5_9_9_...\n",
      "  N-gram 1: 5_8_5_2_6_6_6_7_6_6_3_3_2_2_3_3_6_6_2_2_3_4_5_3_2_5_5_4_4_5_5_5_5_5_5_5_3_3_5_5_5_5_5_7_5_5_5_5_9_9_...\n",
      "  N-gram 2: 5_8_5_2_6_6_6_7_6_6_3_3_2_2_3_3_6_6_2_2_3_4_5_3_2_5_5_4_4_5_5_5_5_5_5_5_3_3_5_5_5_3_5_6_5_5_5_5_7_7_...\n",
      "Generated 43200 n-grams\n"
     ]
    }
   ],
   "source": [
    "n = 2  # n-gram order\n",
    "train_ngrams = generate_ngrams(train_states, n)\n",
    "test_ngrams = generate_ngrams(test_states, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b946641f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Bloom Filter (M=1000000, k=5) ===\n",
      "  Added 1/216000 n-grams\n",
      "  Added 10001/216000 n-grams\n",
      "  Added 20001/216000 n-grams\n",
      "  Added 30001/216000 n-grams\n",
      "  Added 40001/216000 n-grams\n",
      "  Added 50001/216000 n-grams\n",
      "  Added 60001/216000 n-grams\n",
      "  Added 70001/216000 n-grams\n",
      "  Added 80001/216000 n-grams\n",
      "  Added 90001/216000 n-grams\n",
      "  Added 100001/216000 n-grams\n",
      "  Added 110001/216000 n-grams\n",
      "  Added 120001/216000 n-grams\n",
      "  Added 130001/216000 n-grams\n",
      "  Added 140001/216000 n-grams\n",
      "  Added 150001/216000 n-grams\n",
      "  Added 160001/216000 n-grams\n",
      "  Added 170001/216000 n-grams\n",
      "  Added 180001/216000 n-grams\n",
      "  Added 190001/216000 n-grams\n",
      "  Added 200001/216000 n-grams\n",
      "  Added 210001/216000 n-grams\n",
      "Bloom filter training completed!\n"
     ]
    }
   ],
   "source": [
    "bloom_filter = train_bloom_filter(train_ngrams, M=1_000_000, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c761432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Anomaly Detection ===\n",
      "Anomaly rate: 0.8742 (37764/43200)\n"
     ]
    }
   ],
   "source": [
    "anomalies, anomaly_rate = detect_anomalies(test_ngrams, bloom_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "35726447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "Training samples: 216001\n",
      "Test samples: 43201\n",
      "N-gram order: 2\n",
      "Quantization levels: 20\n",
      "Bloom filter size: 1,000,000 bits\n",
      "Hash functions: 5\n",
      "Anomaly rate: 0.8742\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"N-gram order: {n}\")\n",
    "print(f\"Quantization levels: 20\")\n",
    "print(f\"Bloom filter size: 1,000,000 bits\")\n",
    "print(f\"Hash functions: 5\")\n",
    "print(f\"Anomaly rate: {anomaly_rate:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
