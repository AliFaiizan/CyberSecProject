# Study Project – Tasksheet 3  
---

##  Overview

This repository implements **Task 2** and **Task 3 (a & b)** from Tasksheet 3 of the BTU Study Project “Adversarial Machine Learning”.  
The pipeline uses:

- **Latent features** extracted from a VAE (Task 1).  
- **Traditional ML models** (SVM, kNN, RF, LOF, OCSVM, EllipticEnvelope).  
- **CNN classifier** trained on sliding-window latent features.  
- **Ensemble classifiers** (majority, all, random).  
- **Feature-importance analysis** (Task 3a).  
- **Error-overlap analysis** using Venn/UpSet plots (Task 3b).

All experiments strictly follow the required **evaluation scenarios** (Scenario 1–3) and **5-fold CV** generated manually without sklearn.

---

# ---------------------------------------------
#  Task 2 — Execution of Experiments
# ---------------------------------------------

##  1. Input Features

Task 2 uses latent representations saved as `.npy` files from the VAE.  
Labels are regenerated automatically based on the **same sliding-window logic** used in Task 1.

The script responsible for the entire pipeline is:

task2.py


### Functions in Task 2:
- **load_latent_features_and_labels()**  
  Loads the latent feature matrix and creates proper window labels.

- **run_and_save()**  
  Executes each classifier per fold and saves:
  - Fold metrics
  - Predictions (`Predictions_FoldX.csv`)
  - Runtime & memory usage
  - Trained model (`.joblib`)

- **run_cnn_latent()** (from `task2_cnn_latent.py`)  
  Implements the 6-block CNN and tracks:
  - Window creation time  
  - Normalization time  
  - Training time  
  - Prediction time  
  - Memory usage  

---

## 2. Supported Classifiers

| Scenario | Traditional ML | Deep Learning | Ensemble |
|---------|----------------|---------------|----------|
| 1 | OCSVM, LOF, EllipticEnvelope |    —  | Yes |
| 2 | SVM, kNN, RandomForest |     CNN     | Yes |
| 3 | SVM, kNN, RandomForest |    CNN      | YES |

---

## 3. Running Task 2

### Example
```bash
python task2.py --scenario 2 --latent-file vae_features/latent_classification_M20.npy -k 5 -M 20


##  Task 3(a): Feature Importance Analysis

In this task, we evaluate the relative importance of the latent features extracted by the VAE by
computing feature-importance scores for each classifier across all evaluation scenarios (1, 2, and 3).
The analysis is performed using the trained models from Task 2.

###  Methodology

For each scenario and each classifier, the following steps were executed:

1. **Load latent features** generated by the VAE (classification decoder).
2. **Load trained ML models** for all 5 folds from:

saved_models/ScenarioX/<MODEL>_FoldY.joblib

3. **Select the appropriate importance metric**:
- **Random Forest** → `feature_importances_`
- **Linear SVM** → absolute coefficient magnitudes
- **Non-linear / One-Class models** (OCSVM, LOF, EllipticEnvelope, kNN) →  
  **permutation importance**
4. **Compute importance values for each fold**.
5. **Plot per-fold importance** and **aggregated mean ± std importance**.

###  Outputs

Plots are stored under:

feature_importance/ScenarioX/


Each model produces:
- Per-fold importance plots:  
  `Model_FoldY_importance.png`
- Aggregated importance plot (mean ± std):  
  `Model_aggregated_importance.png`

###  Interpretation

- Features with consistently high scores across folds contribute the most to the classifier’s final decision.
- Models may rank features differently depending on:
  - Their sensitivity to variance (SVM/LOF)
  - Their structural bias (RF prefers discrete splits)
- Scenarios 2 and 3 typically show higher separation between important vs. non-important features due to the presence of labeled attack classes.

---

##  Task 3(b): Classification Error Overlap (Venn & UpSet Analysis)

This task analyzes how the different classifiers disagree by comparing their misclassification
errors for each fold of the evaluation scenarios.

For every fold, we compute:

error_set(model) = { i | predicted_label[i] ≠ true_label[i] }


These sets are compared visually to understand whether different classifiers fail on the same
samples or on different types of samples.

---

###  Scenario 1 → Venn Diagram (3 one-class models)

Models:
- OCSVM  
- LOF  
- Elliptic Envelope  

For each fold (1–5), we plot:
venn_scenario1_foldY.png

This reveals:
- Overlap in false positives across one-class detectors
- Unique failures specific to LOF or EE
- Situations where only OCSVM deviates

---

###  Scenarios 2 & 3 → UpSet Plots (multi-model comparison)

Models:
- SVM  
- kNN  
- Random Forest  
- CNN  

UpSet plots are more suitable here since we have 4 classifiers.

For each fold, we produce:

venn_scenario1_foldY.png


These plots show:
- Intersection regions where multiple models misclassify the same samples
- Unique misclassification clusters per classifier
- Whether CNN errors align more with SVM/kNN or with Random Forest

---

###  Why This Analysis Matters

- Helps identify **systematic blind spots** shared across classifiers.
- Shows whether ensemble methods are justified (high disagreement → ensembles help).
- Highlights which models tend to produce **unique vs. overlapping** error patterns.
- Provides insight into **dataset difficulty** and **latent-feature expressiveness**.

---

###  Output Directory

All generated plots are stored under:

task3b_errors/ScenarioX/
